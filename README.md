# BFGS
Текст задания
    * Реализовать алгоритм BFGS
    * Выявить преимущества и недостатки по отношению к методу градиентного спуска
## Теоретическая часть:

Метод BFGS, итерационный метод численной оптимизации, назван в честь его исследователей: Broyden, Fletcher, Goldfarb, Shanno. Относится к классу так называемых квазиньютоновских методов. В отличие от ньютоновских методов в квазиньютоновских не вычисляется напрямую гессиан функции, т.е. нет необходимости находить частные производные второго порядка. Вместо этого гессиан вычисляется приближенно, исходя из сделанных до этого шагов.
Существует несколько модификаций метода:
    * L-BFGS (ограниченное использование памяти) — используется в случае большого количества неизвестных.
    * L-BFGS-B — модификация с ограниченным использованием памяти в многомерном кубе.
Метод эффективен и устойчив, поэтому зачастую применяется в функциях оптимизации. Например в SciPy, популярной библиотеки для языка python, в функции optimize по умолчанию применяется BFGS, L-BFGS-B.

## Ход работы
Реализованы:
* Алгоритм BFGS
* Алгоритм GD
* Замеры времени исполнения кода
* Замеры потребления памяти
* Отслеживается количество итераций
## Вывод
BFGS - это квазиньютоновский метод, который сходится за меньшее количество шагов, чем GD, и имеет немного меньшую склонность к "застреванию" и требует меньших настроек гиперпараметров для достижения значительного снижения на каждой итерации. Каждая итерация может быть совершена со стоимостью  ( плюс стоимость вычисления функции и оценки градиента). Здесь нет  операций таких, как решение линейных систем или сложных математических операций. Алгоритм устойчив и имеет сверхлинейную сходимость. Даже если методы Ньютона сходятся гораздо быстрее (квадратично), стоимость каждой итерации выше, поскольку необходимо решать линейные системы. Неоспоримое преимущество алгоритма, конечно, состоит в том, что нет необходимости вычислять вторые производные.
Методы, подобные GD, дешевле, если матрично-векторные вычисления дешевы, а ваша задача настолько велика, что хранение гессиана затруднено или невозможно. BFGS использует несколько векторных вычислений для обновления своего приблизительного гессиана, поэтому каждая итерация BFGS будет стоить дороже, но вам потребуется меньшее их количество, чтобы достичь локального минимума. 
Формула BFGS имеет самокорректирующиеся свойства. Если матрица  не верно оценивает кривизну функции и если эта плохая оценка замедляет алгоритм, тогда апроксимация гессиана стремится исправить ситуацию за несколько шагов. Самокорректирующие свойства алгоритма работают только в том случае, если реализован соответствующий линейный поиск (соблюдены условия Вольфе).

Непонятно как в дальнейшем поведет себя алгоритм в сравнении с более продвинутыми версиями градиентного спуска, есть вероятность, что те преимущества, объявленные выше, перестанут ими быть, а в сухом остатке будет только скорость работы и потребление памяти. Интересно апробировать алгоритм в реальной задаче обучения нейронной сети, в которой чрезвычайно важна скорость работы и потребление памяти. Судя по тому, что градиентный спуск до сих пор является основным алгоритмом в обучении нейросетей, можно сделать вывод, что BFGS на сегодня "не взлетел" и требует дополнительных исследований.
